Unit 5: Classification and Prediction(9)


    a. What is Classification

            It is a process of assigning data items in a database to predefined classes
            Predicts class of new or unknown records in a dataset
            Assigns data items to predefined classes or groups.
            Predicts the category of new or unknown records.
            Learns from historical data to predict categories for new cases.
            A supervised data mining method (learns from known examples).
            Uses attributes/features of records for classification.
            Examples:
                Bank → Customer risk: High / Medium / Low
                Retail → Customer response: Yes / No
                Medical → Disease: Present / Absent
            Applications:
                Fraud detection: Identify suspicious transactions from past patterns
                Medical diagnosis: Detect diseases using patient records
                Customer segmentation: Group customers for marketing or recommendations



    b. What is Prediction

            - Prediction is the process of forecasting a continuous numeric value for new or unknown records.
            - Estimates the likely outcome based on patterns learned from historical data.
            - It is a supervised data mining technique (like classification) but predicts numeric values instead of categories.
            - Uses attributes/features of records for prediction.
            - Examples:
                Bank → Predict customer’s future balance
                Retail → Estimate sales for next month
                Healthcare → Predict patient’s blood pressure or risk score
            - Applications:
                Stock market forecasting
                Weather prediction
                Sales estimation
                Risk assessment in finance and insurance



    c. Issues Regarding Classification and Prediction

            Data may be incomplete, wrong, or messy
            It is hard to choose which features are important
            Model may fit training data too much and fail on new data (overfitting)
            Model may be too simple and miss patterns (underfitting)
            Some classes may have very few examples, causing bias
            Large data makes processing slow and difficult
            It is difficult to check if the model is really accurate
            Model may need changes when new data comes


    d. Classification by Decision Tree Induction

            - Decision Tree is a tree-like model used to classify data into predefined classes.
            - Each internal node represents a test on an attribute.
            - Each branch represents an outcome of the test.
            - Each leaf node represents a class label (decision).
            - It is easy to understand and interpret.
            - It handles both numeric and categorical data.

            - Decision Tree Induction is a method used to classify data by creating a tree.
            - The tree helps decide which class a new item belongs to by asking simple questions at each step.
            - It is easy to understand.
            - Works well on many types of data.
            - Produces rules that humans can read.

            1. Root Node:  The first and most important attribute used to start the decision.

            2. Internal Nodes:  Middle steps where we ask more questions.

            3. Leaf Nodes:  Final output (class label like Yes/No, Pass/Fail, Buy/Not Buy).

            4. Entropy:  
                    A number that shows how mixed the data is.  
                    Mixed classes → high entropy  
                    Pure classes → low entropy

                    Formula:  Entropy(S) = - Σ p(i)  log2(p(i))

            5. Information Gain:  
                    Tells us which attribute gives the best split.  
                    Higher information gain = better attribute.

                    Formula:  Gain(S, A) = Entropy(S) - Weighted entropy after splitting on A.

            Example idea:
                    Is Age = Youth?
                    → If yes, go right
                    → If no, go left
                    Continue until you reach a final class label.


        Steps of Decision Tree Induction (Iterative Dichotomiser 3 (ID3) Algorithms)

            Step 1: Start with the entire training dataset  
                    Look at all attributes and the class labels.

            Step 2: Calculate entropy of the dataset  
                    This shows how mixed the class labels are.

            Step 3: For every attribute, calculate information gain  
                    Check which attribute gives the best separation of the classes.

            Step 4: Select the attribute with the highest information gain  
                    This becomes the root node of the tree.

            Step 5: Split the data based on that attribute  
                    Each value becomes a branch.

            Step 6: Repeat the same process for each branch  
                    Use remaining attributes.  
                    Calculate entropy again.  
                    Calculate information gain again.  
                    Choose best attribute.  
                    Split again.

            Step 7: Stop when:
                    - All samples in a branch belong to the same class  
                    - No attributes left  
                    - No data left  

                    Create a leaf node with the final class label.

            Step 8: Final Decision Tree  
                    This tree can now classify new data.


    e. Bayes Theorem

            Bayes Theorem is a fundamental formula used in probability and classification.
            It helps to find the probability of a class given some observed data.

            It updates prior knowledge with new evidence.
            Start with what you already believe (prior).
            Add the effect of new information (likelihood).
            Get an updated belief (posterior).
            It forms the foundation of Bayesian Classification.
            It handles the uncertainty using probability.
            It works well even with small datasets.
            It provides clear mathematical reasoning for predictions.


            Formula:
                P(C | X) = [ P(X | C)  P(C) ] / P(X)

            Where:
                - P(C | X): Posterior probability  
                    Probability of class C after observing data X.
                
                - P(X | C): Likelihood  
                    Probability of observing X if the class is C.
                
                - P(C): Prior probability  
                    Original probability of class C before seeing the data.
                
                - P(X): Evidence  
                    Overall probability of observing X (acts as a normalizing term).

        
        
        f. Naive Bayesian Classification
                - Naive Bayesian Classification is a probability-based method used to classify data into predefined classes.
                - It uses Bayes Theorem to calculate how likely a record belongs to each class.
                - It assumes that all attributes are independent of each other.
                - It learns the probabilities of attribute values for each class from training data.
                - For new records, it selects the class with the highest calculated probability.
                - It is widely used in text classification, spam detection, recommendation systems, and medical diagnosis.

                        Prior Probability:
                                The original probability of each class before considering any attributes.

                        Likelihood:
                                The probability of each attribute value given a specific class.

                        Posterior Probability:
                                The final probability of a class after combining prior and likelihood.

                        Independence Assumption:
                                Each attribute contributes separately even if they are related in real life.

                Formula:
                        For attributes X = (x1, x2, x3, …, xn) and class C:

                                P(C | X) = [ P(C) × P(x1 | C) × P(x2 | C) × ... × P(xn | C) ] / P(X)

                        - Since P(X) is constant for all classes, we compare only:

                                P(C) × P(x1 | C) × P(x2 | C) × ... × P(xn | C)

                        - The class with the highest value is selected.

                Steps of Naive Bayesian Classification:
                        1. Collect the training dataset with attributes and class labels.
                        2. Calculate the prior probability for each class.
                        3. Calculate the likelihood probability for each attribute value under each class.
                        4. Multiply prior and likelihood probabilities for each class.
                        5. Choose the class with the highest result as the prediction.

                Simple Example Idea:
                        To classify if a customer will Buy a product:
                                - Calculate P(Buy = Yes) and P(Buy = No)
                                - Calculate attribute likelihoods:
                                        P(Age = Youth | Yes), P(Income = High | Yes), etc.
                                - Multiply values using the Naive Bayes formula
                                - Compare results
                                - The class with the larger probability is the predicted output


        5.6 Classification by Backpropagation
                - Backpropagation is a supervised learning method used to train neural networks, especially multilayer networks.
                - The network learns patterns from labeled training data and uses them to classify new inputs.
                - It adjusts internal weights step-by-step based on the error between actual and predicted outputs.
                - The process starts with a forward pass, where input flows through the network to produce an output.
                - The error is then calculated and sent backward to update the weights.
                - The goal is to gradually reduce this error and improve classification accuracy.
                - After training, the network can classify new, unseen data based on the patterns it has learned.
                - Backpropagation is widely used in image recognition, speech processing, handwriting detection, and pattern classification.


                How Classification Happens:
                        - During training, each input is paired with its correct class.
                        - The network tries to predict the class, compares it with the correct class, and calculates error.
                        - Backpropagation updates weights to reduce this error.
                        - After enough training, the network learns the patterns of each class.
                        - When a new input comes:
                                1. It passes forward through the layers.
                                2. The output layer produces scores.
                                3. The highest score determines the class.
                        - Example:
                                Output layer gives → [Class1=0.1, Class2=0.8, Class3=0.1]
                                Predicted class = Class 2

                Steps in Backpropagation:
                        1. Forward Pass:
                        - Input layer sends features to hidden layer(s).
                        - Hidden layer processes and passes signals to output layer.
                        - Output layer gives predicted class score(s).

                        2. Error Calculation:
                        - Error = Actual Output – Predicted Output
                        - Shows how wrong the prediction is.

                        3. Backward Pass:
                        - Error moves backward through the network.
                        - Each weight is updated to reduce future errors.

                        4. Weight Adjustment:
                        - Uses Gradient Descent method.
                        - Large errors cause larger weight changes.
                        - Errors gradually reduce as training continues.

                When Training Stops:
                        - When total error becomes very small.
                        - Or when weight updates become tiny.
                        - Or when the maximum training cycles are completed.


        5.6.1  A Multilayer Feed Forward Neural Network
                - A Multilayer Feed Forward Neural Network is a type of artificial neural network where:
                  Information flows in one direction—from input layer → hidden layer(s) → output layer.
                - There are no cycles or loops in the network (hence "feed forward").
                - Each neuron in a layer is connected to all neurons in the next layer with an associated weight.
                - Activation functions (like Sigmoid, ReLU, Tanh) are applied at neurons to introduce non-linearity.
                - During training, Backpropagation is used to adjust weights to minimize the error between predicted and actual outputs.
                - The network learns to classify input data based on learned patterns.

                Layers:
                        - Input Layer: Accepts features/attributes of the data.
                        - Hidden Layer(s): Perform computations and extract patterns.
                        - Output Layer: Produces final output for classification or prediction.
                
                Applications:
                        - Image recognition
                        - Speech processing
                        - Handwriting detection
                        - Pattern classification


        5.6.2 Defining a Network Topology

                Network Topology is the layout or structure of a neural network.  
                It shows how neurons are arranged in layers, how many layers there are, 
                and how the neurons are connected to each other.
                Start with one hidden layer and adjust neurons based on performance.
                Proper topology ensures effective learning and accurate classification.

                Steps to define a network topology:
                        1. Input Layer:
                                - Number of neurons = number of features/attributes in the dataset.
                                - Each neuron represents one input variable.
                        2. Hidden Layer(s):
                                - One or more hidden layers may be used.
                                - Number of neurons is chosen based on complexity of problem.
                                - Too few neurons → network cannot learn patterns (underfitting).
                                - Too many neurons → network may memorize data (overfitting).
                        3. Output Layer:
                                - Number of neurons = number of classes (for classification) or 1 (for regression).
                        4. Activation Functions:
                                - Sigmoid, ReLU, or Tanh functions are applied in hidden/output layers to introduce non-linearity.
                        5. Connections and Weights:
                                - Each neuron in a layer is connected to every neuron in the next layer.
                                - Weights are assigned to each connection and updated during training.
                

        5.7 Classification Based on Concept from Association Rule Mining

                - This classification method uses patterns or rules discovered from association rule 
                  mining to classify new data.  
                - Instead of building a decision tree or neural network, it relies on frequent itemsets 
                  and association rules extracted from historical data.
                - Market basket analysis → predict likely product purchase categories.
                - Medical diagnosis → classify patient cases based on symptom patterns.
                - Customer segmentation → assign customers to groups based on buying behavior.
                        - Identify frequent patterns in the dataset.
                        - Generate association rules from these patterns.
                        - Use these rules to predict the class of new or unknown records.

                Steps used in classification using association rules:
                        1. Frequent Pattern Mining:
                                - Identify frequently occurring combinations of attribute values in the dataset.
                        2. Rule Generation:
                                - Generate association rules of the form: IF conditions THEN class.
                                - Each rule has support and confidence metrics.
                        3. Rule Selection:
                                - Select strong rules with high support and confidence.
                                - Resolve conflicts if multiple rules match a record.
                        4. Classification:
                                - Apply the selected rules to classify new data.
                
                Methods used in Step 4 (Classification):

                        1. Highest Confidence Rule:
                                Select the rule with the greatest confidence value.

                        2. Weighted Voting:
                                Combine votes of multiple matching rules using weights based on support or confidence.

                        3. Majority Voting:
                                Each rule votes for a class, the class with most votes is selected.

                        4. Rule Ordering:
                                Rules are sorted by confidence or support. 
                                The first matching rule is applied.

                        5. Class Probability Method:
                                Choose the class with the highest summed confidence from all matching rules.


        5.8.1 K-Nearest Neighbour (K-NN) Classifiers
        
                - K-Nearest Neighbour (K-NN) is a simple, supervised learning algorithm used for classification.  
                - It classifies a new data point based on the majority class among its K nearest neighbors in the feature space.
                - It is instance-based learning: the algorithm does not build an explicit model.
                - Classification depends on distance metric and the choice of K.
                - Works well with small to medium datasets.
                - It is sensitive to scaled features; feature scaling (normalization) is recommended.

                How it works:
                        1. Choose the value of K (number of nearest neighbors to consider).
                        2. Calculate the distance between the new data point and all points in the training dataset.
                                distance measures: 
                                        a. Euclidean Distance:
                                        - Measures the straight-line distance between two points in space.
                                        - Formula: √((x2 - x1)^2 + (y2 - y1)^2 + …)
                                        - Works best when all features are on a similar scale.
                                        - Sensitive to outliers, as a large difference in one feature can dominate the distance.
                                        - It is mainly used for continuous numerical data.

                                        b. Manhattan Distance:
                                        - It measures the total distance traveled along axes at right angles, like moving on a grid.
                                          Formula: |x2 - x1| + |y2 - y1| + …
                                        - Useful when movements are restricted to horizontal or vertical directions.
                                        - Less sensitive to outliers compared to Euclidean distance.
                                        - Suitable for high-dimensional data where individual feature differences matter.

                                        c. Minkowski Distance:
                                        - It provides a generalized distance metric that can represent both Euclidean and Manhattan distances.
                                          Formula: (|x2 - x1|^p + |y2 - y1|^p + …)^(1/p)
                                                If p = 1 → behaves like Manhattan distance; 
                                                if p = 2 → behaves like Euclidean distance.
                                        - Allows flexibility to choose the distance metric by adjusting the parameter p.
                                        - Can be adapted for different types of datasets or problem requirements.

                        3. Identify the K nearest neighbors (points with the smallest distances).
                        4. Determine the majority class among these K neighbors.
                        5. Assign the new data point to that class.

                Applications:
                        - Handwriting recognition
                        - Image classification
                        - Medical diagnosis
                        - Recommender systems


        5.8.2 Genetic Algorithm
                - Genetic Algorithm (GA) is a search-based optimization and classification technique 
                  inspired by natural selection and genetics.  
                - It is used to find optimal or near-optimal solutions for classification and other 
                  data mining problems.
                - GA searches for the best classification rules or parameters automatically.
                - Can handle complex, nonlinear, and large datasets.
                - It is robust against getting stuck in local optima due to mutation and crossover.

                Working methods of Genetic Algorithm
                        1. Initialization:
                        - Start with a population of candidate solutions (chromosomes), often represented as binary strings.
                        - Each candidate represents a possible solution for the classification problem.
                        - Random initialization ensures diversity in the initial population.

                        2. Fitness Evaluation:
                        - Evaluate each candidate based on a fitness function (how well it classifies data).
                        - Higher fitness means a candidate is better at solving the problem.
                        - Helps in identifying which candidates should be used to produce the next generation.

                        3. Selection:
                        - Select the best-performing candidates to produce offspring.
                        - Candidates with higher fitness have a higher chance of being selected.
                        - Ensures that good solutions are preserved and passed to the next generation.

                        4. Crossover:
                        - Combine pairs of candidates to create new offspring by exchanging parts of their chromosomes.
                        - Mimics natural reproduction to create potentially better solutions.
                        - Introduces new combinations of features for exploration.

                        5. Mutation:
                        - Randomly alter some parts of chromosomes to maintain diversity in the population.
                        - Prevents the algorithm from getting stuck in local optimum.
                        - Helps explore new solutions that may not appear through crossover alone.

                        6. Replacement:
                        - Replace less fit candidates with new offspring.
                        - Keeps the population size constant while improving overall fitness.
                        - Ensures that the population evolves toward better solutions over generations.

                        7. Iteration:
                        - Repeat steps 2–6 for multiple generations until an optimal or satisfactory solution is found.
                        - Gradually, the population evolves to produce highly fit solutions.
                        - Stops when a predefined number of generations is reached or fitness reaches a threshold.


        5.8.3 Rough Set Approach
                - The Rough Set Approach is a mathematical framework for handling imprecise or uncertain data. 
                - It is used for classification and to find the hidden pattern in any data set. 
                - It is fully data-driven and does not require prior knowledge such as probability 
                  distributions or membership degrees.  
                - Rules generated are easy to understand and apply.  
                - Important attributes are automatically identified through reducts.  
                - No external parameters or assumptions are needed.  
                - The boundary region explicitly captures uncertain or ambiguous cases.  

                Process:
                        1. Information System and Decision Table  
                        - Data is represented as a table with rows as objects (e.g., patients, customers) 
                          and columns as attributes.  
                        - A Decision Table is a table where one attribute is the decision attribute (the class label to predict).  
                        - Organizes data to facilitate rule extraction and classification.  

                        2. Similarity Grouping  
                        - Objects are grouped into equivalence classes based on similarity in attribute values.  
                        - Objects in the same equivalence class are indistinguishable regarding the selected attributes.  
                        - Reducts are minimal subsets of attributes that preserve the classification ability of the full set.  
                        - Core is the set of indispensable attributes that cannot be removed without losing information.  

                        3. Set Approximation  
                        - Some target concepts cannot be defined precisely, so they are "rough".  
                        - Rough Sets define two approximations:  
                                - Lower Approximation: Objects that definitely belong to the concept.  
                                - Upper Approximation: Objects that possibly belong to the concept.  
                        - Boundary Region = Upper Approximation − Lower Approximation  
                        - The boundary region contains objects that cannot be classified with certainty.  
                        - This directly represents the uncertainty in the data.  

                        4. Rule Generation  
                        - Decision rules are derived from the lower approximation and reducts.  
                        - Rules have the form: IF (attribute conditions) THEN (decision class).  
                        - Rules from the lower approximation are certain, while rules from the boundary region are possible or approximate.  
                        - Rules are human-readable and can be directly applied for classification.  

                        5. Classification and Validation  
                        - New objects are classified by matching them against the generated rule set.  
                        - Conflicts (multiple rules firing) are resolved using:  
                                - Rule strength (confidence)  
                                - Specificity (more precise rules)  
                                - Voting or majority approach  


                Reducts:
                        - Reducts are the smallest set of attributes that can classify the data as effectively as using all attributes.
                        - Think of Reducts as different “paths” that can lead to the correct classification.
                        - They help reduce data dimensionality while retaining classification power.
                        1. Reducts remove redundant attributes, simplifying the dataset.
                        2. Multiple reducts can exist for the same dataset; choosing one depends on the application.
                        3. Reducts help improve computational efficiency because fewer attributes need to be analyzed.
                        4. They preserve the ability to generate the same decision rules as the full attribute set.
                        5. Reducts are automatically identified using Rough Set algorithms, without prior knowledge or assumptions.

                        How Reducts are Calculated:
                                1. Start with the full set of attributes from the decision table.
                                2. Check which attributes are essential for distinguishing objects in different decision classes.
                                3. Remove redundant attributes one by one and see if the classification ability remains the same:
                                - If removing an attribute does not change classification, it is redundant.
                                - If removing it decreases classification accuracy, it is essential.
                                4. Continue this process until no more attributes can be removed without losing classification power.
                                5. Result: The remaining minimal set of attributes is a Reduct.

                Core:
                        Core is the set of indispensable attributes that appear in all possible reducts.
                        Core is the “road that all paths must go through” — it is unavoidable.
                        It identifies attributes that are absolutely necessary for classification.
                        If an attribute is in the Core, removing it will always reduce the classification ability, no matter which reduct you choose.
                        1. Core attributes cannot be removed without losing essential information.
                        2. They are critical for understanding which features truly affect decisions.
                        3. Core provides a foundation for generating decision rules.
                        4. If the core is large, the dataset may have less redundancy; if small, many attributes are optional.
                        5. Core helps in feature selection and explains the importance of attributes to humans clearly.

                        How Core is Calculated:
                                1. Identify all possible reducts of the dataset.
                                2. Find the common attributes that appear in every reduct.
                                3. hese common attributes form the Core, as they are indispensable for classification.



                                        
        5.8.4 Fuzzy Set Approaches

                Fuzzy Set Approaches are used for classification when data is uncertain, or overlapping.  
                Unlike normal classification where an object either belongs or does not belong to a class, 
                fuzzy classification allows partial membership in multiple classes.
                Can deal with uncertain or unclear data easily.  
                Works well for real-world problems that are complicated.  
                Membership rules need to be set carefully for each feature.  
                Best for situations where classes are not clearly separated.

                1. Fuzzy Sets and Membership Functions  
                - Each object can belong to multiple classes with different degrees of membership, between 0 and 1.  
                - Example: A patient may belong to:
                        - "High Risk" → 0.7  
                        - "Medium Risk" → 0.3  
                - Membership functions define how feature values (like age, blood pressure) are converted into these membership values.  
                - This helps handle uncertain or overlapping data naturally.

                2. Fuzzy Rule-Based Classification  
                - Uses IF–THEN rules to classify objects:  
                IF (attribute conditions) THEN (class membership)  
                Example:  
                        IF temperature is High AND blood pressure is High THEN Risk is High (0.8)  
                        - Rules can handle overlapping classes, allowing more flexible and realistic classification.

                3. Fuzzy Inference Process  
                - Combines all fuzzy rules to calculate the overall degree of membership for each class.  
                - Max–min composition → takes the minimum membership value in a rule and maximum across rules.  
                - Weighted averaging → averages membership values with weights.  
                - The object is then assigned to the class with the highest membership degree.  
                - Example: If a patient has 0.6 membership in High Risk and 
                                0.4 in Medium Risk → classified as High Risk.

                4. Defuzzification  
                - Converts fuzzy results into crisp, single decisions when required.  
                - It ensures that fuzzy results can be used for practical decision-making.
                - Example: Assign a patient to the class with the highest membership value.  

                Applications
                        - Medical diagnosis → classify patients into risk categories.  
                        - Image processing → detect objects with fuzzy or unclear boundaries.  
                        - Weather prediction → handle uncertain environmental data.  
                        - Customer profiling → assign customers to overlapping segments.




.

