Unit 3 : Apriori Algorithm


1. Association Rule Mining
    It is a method to find meaningful patterns and relationships between items in large databases.
    It help businesses find what products are often bought together.
    Useful for recommendations, promotions, and understanding customer behavior.

    The main idea: 
        discover rules like “If A happens, B is likely to happen.”
        Rules are shown as X → Y where X and Y are sets of items.
        Support measures how frequently the combined items appear together.
        Confidence measures how often Y occurs when X occurs. 
    The process:
        Find all frequent itemsets that meet minimum support.
        Generate strong association rules from these itemsets with high confidence.
    


2. Market Basket Analysis: Mining a Roadmap
    Market Basket Analysis shows which products are often bought together by customers.
    It is also called frequent itemset mining or association analysis.
    It helps stores understand what items are linked in shopping baskets.
    This helps stores sell more and make customers happy.
    Algorithms like Apriori help do this fast on big data.
    Example: IF someone buys green tea, they often buy honey too.ppppppppppp
             The goal is to find strong rules that tell how items relate to each other.

    Steps in Market Basket Analysis:
            Collect data: Get records of what customers bought together.
            Find frequent itemsets: Find groups of items that appear together a lot.
            Make rules: Write rules like “If item A is bought, item B is likely bought too.”
            Check rules: Use numbers called support and confidence to keep only good rules.
            Use results: Put related products close, give discounts, and manage stock better.



3. The Apriori Algorithm: 

    a.Finding Frequent Itemsets Using Candidate Generation

        The Apriori algorithm helps find frequent itemsets in big databases.
        It works by generating candidate itemsets and checking which ones appear often enough.
        It help businesses understand which products are often bought together.
        It is widely used because it is simple and effective for many types of data.
        The key idea: 
                    If an itemset is frequent, then all its subsets must also be frequent.
                    This idea helps reduce the number of itemsets to check, saving time.

        How Apriori works step-by-step:
                Start with single items and count how often each appears in the data.
                Keep only the items that meet a minimum support (appear often enough).
                Generate new candidates by combining the frequent items from the previous step.
                Count the support for these new candidates in the database.
                Repeat the process until no new frequent itemsets are found.
                After finding frequent itemsets, Apriori can generate strong association rules from them.
        


    b. Generating Association Rules from Frequent Itemsets

        After finding frequent itemsets using Apriori algorithm, the next step is to create association rules from those itemsets.
        An association rule is written as: X → Y, meaning if items in set X are bought, then items in set Y are likely bought too.
        We only generate rules from itemsets that meet the minimum support (frequent enough).
        For each frequent itemset, we divide it into two parts:
            (X): The items that appear first (IF part).
            (Y): The items that appear together with antecedent (THEN part).

        We calculate confidence for each rule:
            Confidence = (Support of X and Y together) / (Support of X alone)

            Confidence shows how often the rule is true. 
            High confidence means rule is reliable.

        We also consider support of the rule: 
            how often X and Y appear together in all transactions.

        Sometimes lift is used 
            to measure how much more often Y appears with X than expected by chance.
            Lift > 1 means positive association between X and Y.

        Generating rules involves:
            Selecting a frequent itemset.
            Dividing it into all possible pairs of subsets (X and Y).
            Calculating confidence and support for each rule.
            Keeping the strong rules that meet confidence and support limits.
        


4. Improving the Efficiency of Apriori
    Apriori can be slow because it generates many candidate itemsets and scans the database multiple times.
    Improving efficiency is important because large databases have millions of transactions.

    To improve speed and reduce work, several techniques are used:

        Reduce candidate generation:
            Use the Apriori property: 
                if any subset of an itemset is not frequent, the itemset can be skipped.
                This cuts down the number of candidates to check.
        
        Transaction reduction:
            Remove transactions that don’t contain any frequent itemsets during later scans to reduce data size.
        
        Partitioning:
            Split the database into smaller parts, find frequent itemsets in each, and combine results.
            This reduces the memory needed and speeds up processing.

        Sampling:
            Analyze a random sample of the database to find frequent itemsets, then check them against the full data.
            This speeds up mining but may miss some itemsets.
        Dynamic itemset counting:
            Add new candidate itemsets while scanning the database instead of waiting for the next pass.
            This reduces the number of database scans.



6. Mining Frequent Itemsets Without Candidate Generation
    Traditional Apriori generates many candidate itemsets, which slows down the process.
    Mining without candidate generation is useful for big data and real-time analysis.
    Some newer algorithms find frequent itemsets without generating candidates explicitly.
    The main example is the 

        FP-Growth algorithm (Frequent Pattern Growth)
            The algorithm extracts frequent itemsets directly from the FP-tree without candidate generation.
            This avoids multiple database scans and reduces the number of itemsets to check.
            FP-Growth is much faster and uses less memory than Apriori for large datasets.
            It is especially good when the dataset has many frequent patterns
        
        Working algorith of FP-Growth.
            Scan database → Count item frequencies and remove infrequent items (below min-support).
            Order items → Sort each transaction’s items in descending frequency order.
            Build FP-tree → Insert ordered transactions into the tree, sharing common prefixes.
            Mine frequent patterns:
                Start from each item (lowest frequency first).
                Create its conditional pattern base (sub-transactions ending with that item).
                Build a conditional FP-tree from this base.
                Recursively mine the conditional FP-tree for more patterns.
            Combine results → Merge all found patterns to get the complete set of frequent itemsets.



7. Multilevel Association Rule
    Multilevel association rules find patterns at different levels of abstraction or detail in the data.
    For example, 
        items can be grouped by category, brand, or specific product.
        Rules can be discovered at 
            a high level (like “If a customer buys any fruit, they also buy milk”) 
            or a detailed level (like “If a customer buys Fuji apples, they also buy goat cheese”).

    Multilevel rules help businesses analyze data more flexibly and get more useful insights.
    Mining at multiple levels can reveal general patterns as well as specific ones.
    Different support thresholds are often used at each level because higher-level groups tend to appear more frequently.
    Multilevel rules help in better targeting marketing and inventory decisions.



8. Approaches to Mining Multilevel Association Rules
    There are several ways to mine association rules at multiple levels of abstraction.
    Main approaches are:

    1. Apriori-Based Approach
        Extends the traditional Apriori algorithm to handle multiple levels of abstraction.
        Works level by level, starting from the top (general) level to the bottom (detailed) level.
        Uses different minimum support thresholds for each level to handle variations in item frequency.
        Simple and systematic.
        Allows flexibility by using multiple support values.
        Can be complex for very large hierarchies.
        How it works:
            Organize items hierarchically, e.g. Electronics → Mobile → Samsung → Samsung A55
            Start mining from the top level (e.g., “Electronics”).
            Find frequent itemsets using a suitable minimum support.
            Move down one level (e.g., “Mobile,” “Laptop”) and apply a lower support threshold.
            Continue until the lowest level is reached.
        Example:
            If “Electronics” is frequent → explore “Mobile” and “Laptop.”
            If “Mobile” is frequent → explore “Samsung,” “Apple,” etc.
        
    2. Top-Down Progressively Deepening Approach
        Similar to the Apriori-based approach but focuses on efficiency and reducing computation time.
        Only explores deeper levels when frequent itemsets are found at higher levels.
        Avoids unnecessary searches in unpromising areas.
        Saves time and computational effort.
        Efficient for large datasets.
        How it works:
            Start from the highest (most general) level.
            Identify frequent categories or items at that level.
            Go deeper only for those frequent categories.
        Example:
            If “Electronics” is frequent → go deeper into “Mobile,” “Laptop,” etc.
            If “Furniture” is not frequent → skip “Chairs,” “Tables,” etc.
        
    3. Post-Processing Approach
        Works in the reverse direction compared to the top-down methods.
        Starts at the lowest (most detailed) level and then generalizes upward to higher levels.
        Useful when detailed data is already available.
        Helps in summarizing detailed results.
        Useful for reporting and analysis after mining detailed data.
        Can be slow and memory-intensive because it processes all detailed items first.
        How it works:
            First, mine all frequent itemsets at the lowest level (e.g., “Samsung A55,” “HP Pavilion”).
            Then, combine and generalize results to higher levels (e.g., “Samsung Mobiles,” “Electronics”).

        For Example:If “Samsung A55” and “Samsung A35” are frequent → generalize to “Samsung Mobiles.”
    
    4. Pattern-Growth Methods
        Based on the FP-Growth algorithm, adapted for multiple levels.
        Uses a tree structure (FP-tree) to store and mine itemsets efficiently.
        Avoids candidate generation, unlike Apriori-based methods.
        Faster and more memory-efficient.
        Suitable for large and complex datasets.
        Works well for deep hierarchies.
        How it works:
            Build an FP-tree representing items and their frequencies.
            Extend the tree to include hierarchical information (category → subcategory → product).
            Mine frequent patterns directly by growing them from the tree.

        For example: If FP-tree shows frequent pattern {Mobile, Charger} → extend to {Samsung Mobile, Samsung Charger}.


        .
        