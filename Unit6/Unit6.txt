Unit 6: Cluster Analysis(8)

6.1 What is Cluster Analysis?
        A cluster is a group of data objects that are similar to each other and different from objects in other groups. 
        Objects in a cluster share common characteristics based on similarity or distance measures. 
        A good cluster has high similarity within the group and low similarity with other groups.

        Cluster analysis is a data mining technique that groups similar data objects without using predefined labels. 
        It helps to discover natural patterns in data. 
        A cluster is a group of objects that are similar to each other and different from objects in other groups.

        - It is an unsupervised learning method because the algorithm finds groups on its own.
        - It identifies natural patterns or structures in large datasets.
        - Objects in the same cluster have high similarity (intra-cluster similarity), 
          while objects in different clusters have low similarity (inter-cluster similarity).
        - Similarity is measured using distance metrics such as Euclidean distance, Manhattan distance, or Cosine similarity.
        - It helps to reduce data complexity by summarizing large datasets into smaller, understandable groups.
        - It is useful when no class labels exist, making it ideal for exploratory data analysis.
        - It improves decision-making by revealing hidden relationships among data objects.
        - It supports other processes like classification, anomaly detection, and pattern discovery.
        - Cluster analysis is mainly applied in areas like marketing, biology, image processing, 
          network security, medical analysis, document grouping, and recommender systems.
        - The outcome depends on the algorithm used and the nature of the data, 
          so different clustering methods may produce different clusters.
        - It is used in works like customer segmentation, grouping similar documents, 
          finding abnormal behavior, social network grouping, and geographic data analysis (GIS).



6.2 Types of Data in Cluster Analysis

        Cluster analysis works on different types of data, which determine how clustering is performed. 
        The main data types in Cluster Analysis are:

            Interval-Scaled Variables
                - Continuous measurements on a linear scale.
                - Examples: height, weight, temperature, latitude/longitude.
                - Standardization is needed to give equal weight to all variables.
                - Sensitive to scale changes; converting units may affect clustering results.
                - Good for algorithms using distance metrics like K-means.
                - Allows computation of meaningful distances between objects.

            Binary Variables
                - Variables with only 2 possible values.
                - Example: gender (male/female), yes/no responses.
                - Similarity can be measured using:
                    - Simple matching coefficient (for symmetric binary data)
                    - Jaccard coefficient (for asymmetric binary data)
                - Easy to encode and compute similarity.
                - Often used in yes/no, presence/absence, or true/false datasets.
                - Can be combined to create categorical variables if needed.

            Nominal or Categorical Variables
                - Variables with more than 2 categories.
                - Example: colors (red, yellow, blue, green)
                - Dissimilarity can be measured via simple matching or by converting categories into binary variables.
                - No inherent order between categories.
                - Useful for grouping objects with similar labels.
                - Often converted into multiple binary variables for clustering algorithms.

            Ordinal Variables
                - Ordered variables where the rank matters.
                - Example: customer satisfaction ratings (low, medium, high)
                - Can be treated like interval-scaled by mapping ranks to [0,1].
                - Preserves order information but not precise distances.
                - Can be combined with interval-scaled variables after mapping.
                - Useful when relative ranking is important but exact differences are unknown.

            Ratio-Scaled Variables
                - Positive measurements on a nonlinear/exponential scale.
                - Example: population growth
                - Can be transformed using logarithms (log) before clustering.
                - Ratios between values are meaningful (e.g., twice as large).
                - Sensitive to outliers; may require normalization.
                - Suitable for exponential or multiplicative growth data.

            Mixed-Type Variables
                - Databases may contain multiple variable types simultaneously.
                - Clustering algorithms must handle combined data types appropriately.
                - Often requires hybrid approaches like K-prototype.
                - Useful in real-world datasets where data is diverse.
                - It requires careful similarity computation to avoid bias from one type dominating.

        Data Structures used in Cluster Analysis:
                Data Matrix: n objects x p variables (rows = objects, columns = variables)
                - Each row represents an object, each column represents a variable.
                - Suitable for memory-based algorithms like K-means.

                Dissimilarity Matrix: n x n matrix storing pairwise distances or dissimilarities between objects
                - Each entry d(i,j) shows the difference between objects i and j.
                - Useful for hierarchical and density-based clustering methods.




6.3 A Categorization of Major Clustering Methods

        Clustering methods can be broadly categorized based on how they group data points. 
        Choosing the right method depends on the data type, number of clusters, and application. 
        The main clustering methods are:

            Partitioning Methods
                - Divide data into a fixed number of k clusters.
                - Example: K-means clustering.
                - Each data point belongs to exactly one cluster.
                - Works well with large datasets and interval-scaled data.
                - Sensitive to initial cluster centers and outliers.
                - Iteratively assigns points to the nearest cluster center and updates centers.

            Hierarchical Methods
                - Builds a tree-like structure of clusters called a dendrogram.
                - Agglomerative (Bottom-up): Start with each object as a separate cluster and merge step by step.
                - Divisive (Top-down): Start with one cluster and split recursively into smaller clusters.
                - No need to specify the number of clusters in advance.
                - Suitable for small to medium datasets due to high computational cost.
                - Provides a visual representation of cluster relationships.

            Density-Based Methods
                - Form clusters based on high-density regions; low-density points are treated as noise or outliers.
                - Example: DBSCAN (Density-Based Spatial Clustering of Applications with Noise).
                - Can detect clusters of arbitrary shapes.
                - Works well with spatial, geographic, or fraud detection data.
                - Robust to noise and outliers.
                - Requires setting parameters for density threshold and minimum points.

            Grid-Based Methods
                - Divide the data space into a grid structure for faster processing.
                - Clustering is performed on the grid cells rather than individual points.
                - Efficient for very large datasets.
                - Example: STING (Statistical Information Grid).
                - Suitable for spatial and climate data analysis.
                - Reduces computational complexity by limiting distance calculations.

            Model-Based Methods
                - Assumes data is generated from a mixture of underlying probability distributions.
                - Example: Gaussian Mixture Models (GMM).
                - Clusters are formed by estimating the parameters of these distributions.
                - Useful for overlapping clusters and soft clustering.
                - Can handle different cluster shapes and sizes.
                - Often combined with expectation-maximization (EM) algorithm.

            Constraint-Based Methods
                - Uses user-defined constraints to guide clustering.
                - Constraints specify which points must or must not be in the same cluster.
                - Example: Healthcare data clustering considering genetic and lifestyle factors.
                - Ensures domain-specific requirements are met.
                - Can improve clustering quality in complex, real-world data.
                - Useful when prior knowledge about relationships exists.




6.4.1 K-Means Clustering

            K-Means is a classical partitioning method that divides a dataset into k clusters 
            based on the distance of points to cluster centroids. Each data point belongs to exactly one cluster.
            The goal is to maximize intra-cluster similarity and minimize inter-cluster similarity.
            K-Means assumes clusters are roughly spherical and evenly sized.
            Sensitive to:
                - Initial selection of centroids
                - Outliers and noisy data
            It works best with numerical/interval-scaled data.
            Distance metrics (Euclidean or Manhattan) determine how clusters are formed.
            It is simple, fast, and widely used in market segmentation, image analysis, and customer grouping.

        Steps to Perform K-Means Clustering:
                1. Choose the number of clusters, k, based on prior knowledge or methods like the elbow method.
                2. Initialize k cluster centroids randomly from the data points.
                3. Assign each data point to the nearest cluster centroid using a distance metric (e.g., Euclidean distance).
                4. Recalculate the cluster centroids by calculating the mean of all points assigned to each cluster.
                5. Repeat steps 3â€“4 until centroids do not change significantly or a maximum number of iterations is reached.
        
            

6.4.2 K-Medoids Clustering

        K-Medoids is a partitioning clustering method similar to K-Means, 
        but it uses actual data points as cluster centers (medoids) instead of the mean of points. 
        Where,Medoid is the most centrally located object in a cluster.
        K-Medoids is less affected by unusual data points (outliers) compared to K-Means.        
        It produces more meaningful clusters when the dataset has irregular shapes.
        It is slower for large datasets due to repeated distance calculations.
        It is more stable than K-Means clustering.
            
    Steps to Perform K-Medoids Clustering:
            1. Initialize k medoids randomly from the dataset.
            2. Assign each data point to the nearest medoid using a distance metric (e.g., Manhattan distance).
            3. For each cluster, select a new medoid by finding the point that minimizes the total distance to other points in the cluster.
            4. Reassign points to the nearest medoid.
            5. Repeat steps 3-4 until medoids do not change or the algorithm converges.

                    
        Manhattan distance formula: 
                In 2D (for points (x1, y1) and (x2, y2)): D = |x1 - x2| + |y1 - y2|



6.5.1 From K-Medoids to CLARANS

    CLARANS (Clustering Large Applications based on RANdomized Search) is an improvement of K-Medoids designed for large datasets.
    While K-Medoids examines all possible swaps to find the optimal medoids, 
    CLARANS uses a randomized search to reduce calculations and improve scalability.
    It is suitable for large databases where K-Medoids is too slow.
    It uses a random subset of neighbors instead of checking all possible medoid swaps.
    It balances efficiency and accuracy in finding clusters.
    Each iteration chooses a random candidate swap and evaluates if it reduces 
    total clustering cost (sum of distances within clusters).
    Stops when no improvement is found after a fixed number of random swaps.

    Steps to Perform CLARANS:
        1. Select k initial medoids randomly from the dataset.
        2. For a fixed number of iterations, randomly select a medoid and a non-medoid data point.
        3. Swap the medoid with the non-medoid and calculate the total clustering cost.
        4. Keep the swap if it reduces the cost; otherwise, discard it.
        5. Repeat steps 2-4 until a local optimum is reached or maximum iterations are done.
        6. The medoids at the end define the final clusters.
